import { Mermaid } from '../components/mermaid'

GreenThread is a GPU inference platform that lets you serve dozens of large language models on a single node by intelligently swapping them in and out of GPU memory. Instead of dedicating GPUs to individual models, GreenThread keeps sleeping models resident in CPU memory and wakes them on demand — in under 2 seconds.

## How it works

GreenThread sits between your application and the inference backends. When a request arrives for a model:

1. If the model is **serving**, the request is forwarded directly — no overhead.
2. If the model is **sleeping**, GreenThread wakes it by restoring weights from CPU cache to GPU, then forwards the request.
3. If GPUs are full, the platform's [fairness policy](/fairness-policy) preempts the least-recently-used model to make room.

All of this happens transparently behind an OpenAI-compatible API.

## Architecture

<Mermaid chart={`
flowchart LR
    subgraph Clients
        C1["Llama 3 8B request"]
        C2["Mistral 7B request"]
        C3["Qwen 14B request"]
    end

    subgraph GreenThread
        API["API + Controller"]
        STR["Storage Server\nCPU weight cache"]
    end

    subgraph GPUs
        G0["GPU 0"]
        G1["GPU 1"]
    end

    C1 --> API
    C2 --> API
    C3 --> API
    API --> G0
    API --> G1
    STR -.->|wake| G0
    STR -.->|wake| G1
`} />

Each GPU runs the active model and holds sleeping models in CPU memory. When a request arrives for a sleeping model, GreenThread swaps it onto a GPU in under 2 seconds — no cold start, no dedicated GPU per model.

## Performance

For a 20B-parameter model:

| Operation | Time |
|-----------|------|
| Traditional cold start | ~40 seconds |
| GreenThread wake (from RAM) | ~1.8 seconds |
| GreenThread sleep | ~90 milliseconds |

This ~22x improvement comes from keeping the inference runtime alive while sleeping and storing weights in CPU memory for fast GPU transfers.

## API compatibility

GreenThread exposes an OpenAI-compatible API. Any SDK or tool that works with OpenAI's API works with GreenThread:

- `POST /v1/chat/completions` — Chat completions with streaming
- `POST /v1/completions` — Text completions
- `GET /v1/models` — List available models with status

## Quickstart

Point your OpenAI SDK at your GreenThread instance:

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://your-greenthread-host:8080/v1",
    api_key="gthread_your_api_key",
)

response = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct",
    messages=[{"role": "user", "content": "Hello!"}],
)
print(response.choices[0].message.content)
```

If the model is sleeping, GreenThread will automatically wake it, serve the request, and return the response — all behind the standard API.

## Next steps

- [Authentication](/authentication) — Set up API keys
- [Inference API](/inference-api) — Full endpoint reference
- [Model Configuration](/model-configuration) — Tune inference settings
- [Adding & Removing Models](/adding-removing-models) — Manage your model fleet
